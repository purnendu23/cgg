{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded feature set(no variation):\n",
      "['RESTART', 'ATTR02', 'ATTR07', 'RESENTSHOTS', 'NTHREADS', 'ATTR09', 'ATTR11', 'ATTR12', 'ATTR13', 'ATTR14', 'ATTR15', 'ATTR16', 'ATTR19', 'ATTR22', 'ATTR23', 'ATTR24', 'ATTR26', 'ATTR27', 'ATTR28', 'ATTR29', 'ATTR20.1', 'ATTR31', 'ATTR32', 'ATTR38', 'ATTR39', 'ATTR30', 'ATTR31.1', 'ATTR32.1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_FOLDER=\"./data/\"\n",
    "alldatafile = \"allData_pkaul.csv\"\n",
    "train_file = \"trainData_pkaul.csv\"\n",
    "test_file = \"testData_pkaul.csv\"\n",
    "\n",
    "train_in = pd.read_csv(os.path.join(DATA_FOLDER, train_file))\n",
    "test_in = pd.read_csv(os.path.join(DATA_FOLDER, test_file))\n",
    "all_in = pd.read_csv(os.path.join(DATA_FOLDER, alldatafile))\n",
    "\n",
    "\n",
    "# remove all features with no variation at all\n",
    "df = all_in.std().reset_index()\n",
    "df.rename(columns={'index':'feature', 0:'stddev'}, inplace=True)\n",
    "exclude_list = list(df[df.stddev==0].feature.unique())\n",
    "exclude_list_add = ['RESTART', 'ATTR02', 'ATTR07']\n",
    "\n",
    "print(\"Excluded feature set(no variation):\\n{}\".format(exclude_list_add + exclude_list))\n",
    "\n",
    "train_in.drop(columns=exclude_list_add + exclude_list, inplace=True)\n",
    "test_in.drop(columns=exclude_list_add + exclude_list, inplace=True)\n",
    "all_in.drop(columns=exclude_list_add + exclude_list, inplace=True)\n",
    "\n",
    "train_in.set_index('ID', inplace = True)\n",
    "test_in.set_index('ID', inplace = True)\n",
    "all_in.set_index('ID', inplace = True)\n",
    "\n",
    "train_in.columns = [c.lower() for c in train_in.columns]\n",
    "test_in.columns = [c.lower() for c in test_in.columns]\n",
    "all_in.columns = [c.lower() for c in all_in.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57636, 23)\n",
      "Index(['node_minutes', 'nodes', 'executionstart', 'shots', 'scname', 'ncpu',\n",
      "       'attr01', 'attr03', 'attr04', 'attr05', 'attr06', 'attr08', 'attr10',\n",
      "       'attr17', 'attr18', 'attr20', 'attr21', 'attr25', 'attr33', 'attr34',\n",
      "       'attr35', 'attr36', 'attr37'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_in.shape)\n",
    "print(train_in.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "1. attr08 can possibly be made a catg-variable becasue most of the values are zeros and besides that there are very few unique vales- 'binning'\n",
    "2. attr06 has strong linear correlation to target. We will just normalize it and keep it.(#sns.lineplot(train_in.attr06, y=train_in.node_minutes))\n",
    "3. attr10 seems to have one outlier which can be ommitted   #(train_in[train_in.attr10<max(train_in.attr10)].attr10.hist(bins = 50) )\n",
    "4. 'attr21', 'attr25', 'attr20', 'attr18', 'attr17' -> scale and use\n",
    "5. 'attr01', 'attr03', 'attr04','attr05' - make these catg variables - one hot encoding\n",
    "6. nodes, shots -> numeric\n",
    "7. scname -> catg\n",
    "\n",
    "\n",
    "Additional\n",
    "\n",
    "8. Consider making attr36 catgs -> binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in.executionstart = pd.to_datetime(train_in.executionstart)\n",
    "train_in['hour'] = train_in.executionstart.apply(lambda dt: dt.time().hour)\n",
    "train_in['peher'] = pd.cut(train_in.hour, bins=[-0.2, 6, 12, 18, 24], labels=['peher1', 'peher2', 'peher3', 'peher4'])\n",
    "train_in['peher'] = train_in.peher.astype(str)\n",
    "train_in['day_of_week'] = train_in.executionstart.apply(lambda dt: dt.isocalendar()[1])\n",
    "\n",
    "train_in['attr08_catg'] = train_in.attr08.apply(lambda i: attr08_binning(x=i))\n",
    "train_in = train_in[train_in.attr10 < max(train_in.attr10)]\n",
    "\n",
    "train_in.attr34 = train_in.attr34/1000000000\n",
    "\n",
    "num_for_scaling = ['attr06', 'attr10','attr21', 'attr25', 'attr20', 'attr18',\n",
    "                   'attr17', 'nodes','shots', 'attr35','attr34', 'attr36', 'attr37']\n",
    "\n",
    "catg_for_ohe = ['attr01', 'attr03', 'attr04','attr05', 'scname','ncpu','attr08_catg', 'day_of_week','peher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test_set\n",
    "\n",
    "test_in.executionstart = pd.to_datetime(test_in.executionstart)\n",
    "test_in['hour'] = test_in.executionstart.apply(lambda dt: dt.time().hour)\n",
    "test_in['peher'] = pd.cut(test_in.hour, bins=[-0.2, 6, 12, 18, 24], labels=['peher1', 'peher2', 'peher3', 'peher4'])\n",
    "test_in['peher'] = test_in.peher.astype(str)\n",
    "test_in['day_of_week'] = test_in.executionstart.apply(lambda dt: dt.isocalendar()[1])\n",
    "\n",
    "test_in['attr08_catg'] = test_in.attr08.apply(lambda i: attr08_binning(x=i))\n",
    "test_in = test_in[test_in.attr10 < max(test_in.attr10)]\n",
    "\n",
    "test_in.attr34 = test_in.attr34/1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer as FT\n",
    "import category_encoders as ce\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import category_encoders as ce\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( train_in, train_in.node_minutes, test_size=0.2,\n",
    "                                                    random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot', ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True) , catg_for_ohe),\n",
    "        ('sqrt_transform', MinMaxScaler(), num_for_scaling)\n",
    "    ]\n",
    ")\n",
    "\n",
    "base_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('lasso', Lasso())   \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# param_grid = {\n",
    "#     'lasso__alpha' : [0.1,0.01,0.001,1]\n",
    "# }\n",
    "\n",
    "# clf = GridSearchCV( base_pipeline, param_grid , cv=cv, scoring='neg_mean_absolute_error')\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_regressor():\n",
    "    regressor = Sequential()\n",
    "    regressor.add(Dense(units=45, input_dim=45, activation='relu'))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(Dense(units=25, activation='relu'))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(Dense(units=10, activation='relu'))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(Dense(units=5, activation='relu'))\n",
    "    regressor.add(Dense(units=1))\n",
    "    regressor.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae'])\n",
    "    return regressor\n",
    "\n",
    "\n",
    "\n",
    "# nn_pipeline = Pipeline([\n",
    "#     ('preprocessing', preprocessor),\n",
    "#     ('NN', regressor)   \n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36886 samples, validate on 9222 samples\n",
      "Epoch 1/25\n",
      "36886/36886 [==============================] - 3s 78us/step - loss: 2.9059 - mean_absolute_error: 1.2931 - val_loss: 3.4217 - val_mean_absolute_error: 1.6322\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.42166, saving model to best_model.h5\n",
      "Epoch 2/25\n",
      "36886/36886 [==============================] - 2s 53us/step - loss: 1.1334 - mean_absolute_error: 0.8201 - val_loss: 2.5942 - val_mean_absolute_error: 1.4275\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.42166 to 2.59417, saving model to best_model.h5\n",
      "Epoch 3/25\n",
      "36886/36886 [==============================] - 2s 57us/step - loss: 0.7433 - mean_absolute_error: 0.6630 - val_loss: 1.4488 - val_mean_absolute_error: 1.0196\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.59417 to 1.44879, saving model to best_model.h5\n",
      "Epoch 4/25\n",
      "36886/36886 [==============================] - 2s 55us/step - loss: 0.5972 - mean_absolute_error: 0.5860 - val_loss: 0.8851 - val_mean_absolute_error: 0.7527\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.44879 to 0.88509, saving model to best_model.h5\n",
      "Epoch 5/25\n",
      "36886/36886 [==============================] - 2s 50us/step - loss: 0.5221 - mean_absolute_error: 0.5423 - val_loss: 0.6156 - val_mean_absolute_error: 0.5869\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.88509 to 0.61556, saving model to best_model.h5\n",
      "Epoch 6/25\n",
      "36886/36886 [==============================] - 2s 55us/step - loss: 0.4711 - mean_absolute_error: 0.5109 - val_loss: 0.5162 - val_mean_absolute_error: 0.5301\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.61556 to 0.51616, saving model to best_model.h5\n",
      "Epoch 7/25\n",
      "36886/36886 [==============================] - 2s 57us/step - loss: 0.4391 - mean_absolute_error: 0.4876 - val_loss: 0.5089 - val_mean_absolute_error: 0.5325\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51616 to 0.50895, saving model to best_model.h5\n",
      "Epoch 8/25\n",
      "36886/36886 [==============================] - 2s 56us/step - loss: 0.4092 - mean_absolute_error: 0.4698 - val_loss: 0.3899 - val_mean_absolute_error: 0.4438\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.50895 to 0.38994, saving model to best_model.h5\n",
      "Epoch 9/25\n",
      "36886/36886 [==============================] - 2s 57us/step - loss: 0.3844 - mean_absolute_error: 0.4530 - val_loss: 0.3254 - val_mean_absolute_error: 0.3993\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.38994 to 0.32542, saving model to best_model.h5\n",
      "Epoch 10/25\n",
      "36886/36886 [==============================] - 2s 57us/step - loss: 0.3637 - mean_absolute_error: 0.4393 - val_loss: 0.4136 - val_mean_absolute_error: 0.4661\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32542\n",
      "Epoch 11/25\n",
      "36886/36886 [==============================] - 2s 55us/step - loss: 0.3444 - mean_absolute_error: 0.4263 - val_loss: 0.3681 - val_mean_absolute_error: 0.4345\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.32542\n",
      "Epoch 12/25\n",
      "36886/36886 [==============================] - 2s 54us/step - loss: 0.3366 - mean_absolute_error: 0.4208 - val_loss: 0.3307 - val_mean_absolute_error: 0.4071\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.32542\n",
      "Epoch 13/25\n",
      "36886/36886 [==============================] - 2s 57us/step - loss: 0.3261 - mean_absolute_error: 0.4143 - val_loss: 0.4181 - val_mean_absolute_error: 0.4844\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.32542\n",
      "Epoch 14/25\n",
      "36886/36886 [==============================] - 2s 58us/step - loss: 0.3184 - mean_absolute_error: 0.4080 - val_loss: 0.4590 - val_mean_absolute_error: 0.4996\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.32542\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3b009978>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "regressor = KerasRegressor(build_fn=build_regressor, batch_size=32,epochs=25)\n",
    "regressor.fit(X_train_preprocessed, np.log(y_train), validation_split=0.2, callbacks=[es, mc], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 395.6440918267916\n",
      "RMSE: 1382.5119204855348\n",
      "r-squared: 0.46642949218226293\n",
      "adj_r-squared: 0.4643381523293061\n"
     ]
    }
   ],
   "source": [
    "clf = load_model('best_model.h5')\n",
    "pred = clf.predict(X_test_preprocessed)\n",
    "\n",
    "\n",
    "R2 = r2_score(y_pred=pred, y_true=y_test)\n",
    "n=len(y_test)\n",
    "p = 45\n",
    "Adj_R2 = 1- (1-R2)*(n-1)/(n-p-1)\n",
    "\n",
    "print(f'MAE: {mean_absolute_error(y_pred=pred, y_true=y_test)}')\n",
    "print(f'RMSE: {np.sqrt(mean_squared_error(y_pred=pred, y_true=y_test))}')\n",
    "print(f'r-squared: {R2}')\n",
    "print(f'adj_r-squared: {Adj_R2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
